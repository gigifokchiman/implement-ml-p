apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-metrics-config
  namespace: ml-platform
  labels:
    app: ml-metrics
data:
  prometheus-ml-rules.yaml: |
    groups:
      - name: ml-platform.rules
        interval: 30s
        rules:
          # Model Performance Metrics
          - alert: ModelAccuracyDrop
            expr: ml_model_accuracy < 0.85
            for: 5m
            labels:
              severity: warning
              component: ml-model
            annotations:
              summary: "ML Model accuracy has dropped below threshold"
              description: "Model {{ $labels.model_name }} accuracy is {{ $value }}, below 85% threshold"
              runbook_url: "https://docs.ml-platform.dev/runbooks/model-accuracy"

          - alert: ModelLatencyHigh
            expr: histogram_quantile(0.95, rate(ml_model_inference_duration_seconds_bucket[5m])) > 0.5
            for: 2m
            labels:
              severity: warning
              component: ml-model
            annotations:
              summary: "ML Model inference latency is high"
              description: "Model {{ $labels.model_name }} 95th percentile latency is {{ $value }}s"

          - alert: ModelErrorRateHigh
            expr: rate(ml_model_predictions_total{status="error"}[5m]) / rate(ml_model_predictions_total[5m]) > 0.05
            for: 3m
            labels:
              severity: critical
              component: ml-model
            annotations:
              summary: "ML Model error rate is high"
              description: "Model {{ $labels.model_name }} error rate is {{ $value | humanizePercentage }}"

          # Data Pipeline Metrics
          - alert: DataPipelineFailed
            expr: increase(ml_pipeline_runs_total{status="failed"}[1h]) > 0
            for: 0m
            labels:
              severity: critical
              component: data-pipeline
            annotations:
              summary: "Data pipeline failure detected"
              description: "Pipeline {{ $labels.pipeline_name }} has failed"

          - alert: DataQualityIssue
            expr: ml_data_quality_score < 0.9
            for: 10m
            labels:
              severity: warning
              component: data-quality
            annotations:
              summary: "Data quality score is low"
              description: "Dataset {{ $labels.dataset_name }} quality score is {{ $value }}"

          - alert: DataIngestionDelayed
            expr: time() - ml_data_last_ingestion_timestamp > 3600
            for: 5m
            labels:
              severity: warning
              component: data-ingestion
            annotations:
              summary: "Data ingestion is delayed"
              description: "Last ingestion for {{ $labels.source }} was {{ $value | humanizeDuration }} ago"

          # Training Metrics
          - alert: TrainingJobStuck
            expr: time() - ml_training_job_start_time > 86400 and ml_training_job_status == 1
            for: 1m
            labels:
              severity: warning
              component: training
            annotations:
              summary: "Training job has been running for over 24 hours"
              description: "Training job {{ $labels.job_id }} has been running for {{ $value | humanizeDuration }}"

          - alert: TrainingJobFailed
            expr: increase(ml_training_jobs_total{status="failed"}[1h]) > 0
            for: 0m
            labels:
              severity: critical
              component: training
            annotations:
              summary: "Training job failed"
              description: "Training job {{ $labels.job_id }} has failed"

          # Resource Utilization
          - alert: GPUUtilizationLow
            expr: avg_over_time(nvidia_gpu_utilization_gpu[15m]) < 10 and ml_training_job_status == 1
            for: 10m
            labels:
              severity: warning
              component: resources
            annotations:
              summary: "GPU utilization is low during training"
              description: "GPU {{ $labels.gpu }} utilization is {{ $value }}% during active training"

          - alert: MLPlatformMemoryPressure
            expr: |
              (
                container_memory_working_set_bytes{namespace="ml-platform"} /
                container_spec_memory_limit_bytes{namespace="ml-platform"}
              ) * 100 > 85
            for: 5m
            labels:
              severity: warning
              component: resources
            annotations:
              summary: "High memory usage in ML Platform"
              description: "Pod {{ $labels.pod }} memory usage is {{ $value }}%"

          # Model Drift Detection
          - alert: ModelDriftDetected
            expr: ml_model_drift_score > 0.3
            for: 5m
            labels:
              severity: warning
              component: ml-model
            annotations:
              summary: "Model drift detected"
              description: "Model {{ $labels.model_name }} drift score is {{ $value }}"
              runbook_url: "https://docs.ml-platform.dev/runbooks/model-drift"

          # Feature Store Metrics
          - alert: FeatureStoreLag
            expr: time() - ml_feature_store_last_update > 1800
            for: 5m
            labels:
              severity: warning
              component: feature-store
            annotations:
              summary: "Feature store update lag"
              description: "Feature {{ $labels.feature_name }} last updated {{ $value | humanizeDuration }} ago"

          - alert: FeatureStoreUnavailable
            expr: up{job="feature-store"} == 0
            for: 1m
            labels:
              severity: critical
              component: feature-store
            annotations:
              summary: "Feature store is unavailable"
              description: "Feature store service is down"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-grafana-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  ml-platform-overview.json: |
    {
      "dashboard": {
        "id": null,
        "title": "ML Platform Overview",
        "tags": ["ml-platform", "machine-learning"],
        "timezone": "browser",
        "panels": [
          {
            "id": 1,
            "title": "Model Predictions/sec",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(rate(ml_model_predictions_total[5m]))",
                "legendFormat": "Predictions/sec"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "reqps"
              }
            },
            "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0}
          },
          {
            "id": 2,
            "title": "Model Accuracy",
            "type": "stat",
            "targets": [
              {
                "expr": "avg(ml_model_accuracy)",
                "legendFormat": "Accuracy"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percentunit",
                "min": 0,
                "max": 1,
                "thresholds": {
                  "steps": [
                    {"color": "red", "value": 0},
                    {"color": "yellow", "value": 0.8},
                    {"color": "green", "value": 0.9}
                  ]
                }
              }
            },
            "gridPos": {"h": 8, "w": 6, "x": 6, "y": 0}
          },
          {
            "id": 3,
            "title": "Inference Latency (P95)",
            "type": "stat",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(ml_model_inference_duration_seconds_bucket[5m]))",
                "legendFormat": "P95 Latency"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "s"
              }
            },
            "gridPos": {"h": 8, "w": 6, "x": 12, "y": 0}
          },
          {
            "id": 4,
            "title": "Training Jobs Status",
            "type": "piechart",
            "targets": [
              {
                "expr": "sum by (status) (ml_training_jobs_total)",
                "legendFormat": "{{status}}"
              }
            ],
            "gridPos": {"h": 8, "w": 6, "x": 18, "y": 0}
          },
          {
            "id": 5,
            "title": "Model Performance Over Time",
            "type": "timeseries",
            "targets": [
              {
                "expr": "ml_model_accuracy",
                "legendFormat": "{{model_name}} Accuracy"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
          },
          {
            "id": 6,
            "title": "Data Pipeline Status",
            "type": "table",
            "targets": [
              {
                "expr": "ml_pipeline_runs_total",
                "legendFormat": "{{pipeline_name}}"
              }
            ],
            "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
          },
          {
            "id": 7,
            "title": "GPU Utilization",
            "type": "timeseries",
            "targets": [
              {
                "expr": "nvidia_gpu_utilization_gpu",
                "legendFormat": "GPU {{gpu}}"
              }
            ],
            "gridPos": {"h": 8, "w": 24, "x": 0, "y": 16}
          }
        ],
        "time": {"from": "now-1h", "to": "now"},
        "refresh": "30s"
      }
    }
---
# ServiceMonitor for ML Platform applications
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ml-platform-metrics
  namespace: ml-platform
  labels:
    app: ml-platform
spec:
  selector:
    matchLabels:
      app: ml-platform-backend
  endpoints:
    - port: metrics
      path: /metrics
      interval: 30s
      scrapeTimeout: 10s
---
# Custom metric collection for ML workloads
apiVersion: v1
kind: Service
metadata:
  name: ml-metrics-exporter
  namespace: ml-platform
  labels:
    app: ml-metrics-exporter
spec:
  type: ClusterIP
  ports:
    - name: metrics
      port: 8080
      targetPort: 8080
      protocol: TCP
  selector:
    app: ml-metrics-exporter
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-metrics-exporter
  namespace: ml-platform
  labels:
    app: ml-metrics-exporter
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ml-metrics-exporter
  template:
    metadata:
      labels:
        app: ml-metrics-exporter
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: metrics-exporter
          image: prom/node-exporter:v1.6.1
          ports:
            - name: metrics
              containerPort: 8080
              protocol: TCP
          env:
            - name: ML_PLATFORM_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 50m
              memory: 64Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: metrics-config
              mountPath: /etc/ml-metrics
              readOnly: true
      volumes:
        - name: metrics-config
          configMap:
            name: ml-metrics-config
---
# PrometheusRule for ML Platform alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ml-platform-alerts
  namespace: ml-platform
  labels:
    app: ml-platform
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
    - name: ml-platform.rules
      interval: 30s
      rules:
        - alert: ModelEndpointDown
          expr: up{job="ml-platform-backend"} == 0
          for: 1m
          labels:
            severity: critical
            component: ml-platform
          annotations:
            summary: "ML Platform backend is down"
            description: "ML Platform backend has been down for more than 1 minute"

        - alert: HighPredictionLatency
          expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{job="ml-platform-backend", handler="/predict"}[5m])) by (le)) > 2
          for: 5m
          labels:
            severity: warning
            component: ml-platform
          annotations:
            summary: "High prediction latency"
            description: "99th percentile prediction latency is {{ $value }}s"

        - alert: ModelLoadFailure
          expr: increase(ml_model_load_errors_total[5m]) > 0
          for: 0m
          labels:
            severity: critical
            component: ml-model
          annotations:
            summary: "Model loading failed"
            description: "Failed to load model {{ $labels.model_name }}"
